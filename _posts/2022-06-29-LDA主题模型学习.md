---
layout: post
title:  "LDA主题模型学习"
subtitle: ""
date: 2022-06-29
author: 七月
category: 技术文档
tags: python
---

LDA 是最受欢迎的主题模型方法之一。其核心思想是：每篇文档都是由若干个主题构成，每个主题由若干个词语构成。LDA的目标是找到每篇文档所属的主题。

已知每个词语所属文档，需要计算每个词语所属的主题，或者说每个主题的词语分布。

基本算法步骤：

1. 对每篇文档的每个单词，随机将其赋值给K个主题中的一个

2. 对每篇文档d，遍历其词语w，并计算：
   $$
   P(t|w)
   $$
   其中，t表示topic， 即计算该词语属于每个topic的概率，又有
   $$
   P(t|w) = P(\frac{t}{d}) * P(\frac{w}{t})
   $$
   所以要计算词语W属于每个topic的概率，需要计算P(t|d)和P(d|w)。其中，d表示文档。p(t|d)表示文档d中属于主题t的词语的比例, P(w|t)表示主题t中词语w所占的比例，然后更新词语w属于文档t的概率。

3. 重复上一步，直到稳定状态

Latent：指的是隐藏主题

Dirichlet：狄雷克利分布

example:

假设语料由如下5篇文档构成：

![image-20220629173507995](/Users/Rosanne/Documents/GitHub/Rosanne-Luo.github.io/img//image-20220629173507995.png)

step1:

随机赋给每个word主题

![image-20220629173629902](/Users/Rosanne/Documents/GitHub/Rosanne-Luo.github.io/img//image-20220629173629902.png)

then, 每个文档的主题构成如下：

![image-20220629173749611](/Users/Rosanne/Documents/GitHub/Rosanne-Luo.github.io/img//image-20220629173749611.png)

每个主题的构成如下：

![image-20220629173834792](/Users/Rosanne/Documents/GitHub/Rosanne-Luo.github.io/img//image-20220629173834792.png)

LDA假设除了当前的词语，其他词语的所属主题都是正确的。

step 2:

for document in documents:

​	for word in document:

​			计算 p(t|w) 更新word的topic分布



LDA的参数：

* M: 文档的数量
* N: 词语的数量
* w：某个词语
* z：tatent topic 
* theta: 主题分布
* LDA模型参数：alpha 和 beta（eta）
  * alpha: document-topic density. alpha 越大，主题数量越多
  * beta: topic word density. beta 越大，每个主题的词语越多

​		最好使用默认的alpha和beta参数，因为模型会自动学习这两个参数



学习资料：

https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2

https://www.analyticsvidhya.com/blog/2021/06/part-2-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/







